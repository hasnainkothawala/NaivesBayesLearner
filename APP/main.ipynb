{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "In order to make the code work paste the FOLLOWING files provided in the aima-data folder attached in the zip file to your AIMA aima-data folder\n",
    "\n",
    "* haberman.csv\n",
    "* audit_risk_data.csv\n",
    "* audit_risk_data_test.csv\n",
    "* nursery_test.csv\n",
    "* nursery.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# configuring to  access the  AIMA repo and access some extras\n",
    "if sys.platform == \"win32\":\n",
    "    module_path = os.path.abspath(os.path.join('..\\..'))\n",
    "    ExtraFolderPath = os.path.abspath(os.path.join('..\\EXTRA_DIR'))\n",
    "else:\n",
    "    module_path = os.path.abspath(os.path.join('../..'))\n",
    "    ExtraFolderPath = os.path.abspath(os.path.join('../EXTRA_DIR'))\n",
    "    \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    sys.path.append(ExtraFolderPath)\n",
    "sys.path.insert(1, module_path) \n",
    "sys.path.insert(2, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries from AIMA and other sources\n",
    "\n",
    "from probability import *\n",
    "from utils import print_table\n",
    "from notebook import psource, pseudocode, heatmap\n",
    "from learning import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Probability Distribution Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Tip1=ProbDist(varname='Tip1',freqs={'Never':1,'Rarely':4, 'Sometimes':6, 'Often':12, 'Always':23})\n",
    "P_Tip2=ProbDist(varname='Tip2',freqs={'Never':12,'Rarely':4, 'Sometimes':12, 'Often':4, 'Always':2})\n",
    "P_Tip3=ProbDist(varname='Tip3',freqs={'Never':24,'Rarely':2, 'Sometimes':5, 'Often':4, 'Always':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Never', 'Rarely', 'Sometimes', 'Often', 'Always']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Tip1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables={ 'Tip1':P_Tip1,'Tip2':P_Tip2,'Tip3':P_Tip3 }\n",
    "\n",
    "# for variable_name,Prob_obj in variables.items():\n",
    "    \n",
    "#     for freq in Prob_obj.values:\n",
    "        \n",
    "#         print('Probability Distribution for {0} for Freq: {1} is {2}'.format(variable_name,freq,Prob_obj[freq]))\n",
    "        \n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Tip1', 0.021739130434782608, 0.08695652173913043, 0.13043478260869565, 0.2608695652173913, 0.5], ['Tip2', 0.35294117647058826, 0.11764705882352941, 0.35294117647058826, 0.11764705882352941, 0.058823529411764705], ['Tip3', 0.6153846153846154, 0.05128205128205128, 0.1282051282051282, 0.10256410256410256, 0.10256410256410256]]\n"
     ]
    }
   ],
   "source": [
    "variables={ 'Tip1':P_Tip1,\n",
    "           'Tip2':P_Tip2,'Tip3':P_Tip3 }\n",
    "prob_dist_table=[]\n",
    "for variable_name,Prob_obj in variables.items():\n",
    "    prob_dist_for_this_feature=[]\n",
    "    prob_dist_for_this_feature.append(variable_name)\n",
    "#     print('Variable Name:',variable_name)\n",
    "#     print('Variable Name from list:',prob_dist_for_this_feature[0])\n",
    "#     print('Prob_obj.values:',Prob_obj.values)\n",
    "    \n",
    "    for freq in Prob_obj.values:        \n",
    "        prob_dist_for_this_feature.append(Prob_obj[freq])\n",
    "#     print('This feature:',prob_dist_for_this_feature)\n",
    "    \n",
    "    prob_dist_table.append(prob_dist_for_this_feature)    \n",
    "print(prob_dist_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   ', 'Rarely', 'Sometimes', 'Often', 'Always']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_Tip1.values[0]='   '\n",
    "P_Tip1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Rarely   Sometimes   Often   Always\n",
      "Tip1     0.02        0.09    0.13     0.26\n",
      "Tip2     0.35        0.12    0.35     0.12\n",
      "Tip3     0.62        0.05    0.13     0.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_table(table=prob_dist_table, header=P_Tip1.values, sep='   ', numfmt='{:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bayesian Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale behind the world view "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* We have been some random variables such as :\n",
    "    ** Artificial Intelligence (AI)\n",
    "    ** Fossil Fuels (FF)\n",
    "    ** Employment (EM)\n",
    "    ** Renewables (RE)\n",
    "    ** Traffic(TR)\n",
    "    ** Global Warming (G)\n",
    "\n",
    "* The rationale that i have used to create and network of these variables and deduce the relations among thme is as below:\n",
    "    ** AI is a independent node and relates to the use of self driving cars in the future.\n",
    "    ** Employed deal with the probability of people being employment(full employement).\n",
    "    ** AI and Employed but contribute the traffic in different ways. More use of AI in self driving cars means less traffic and more employment shall generate more traffic as more and more people shall travel to office and back everyday.\n",
    "    ** Fossil Fuel and Renewables are independent variables and quantify the probability of use of fossil fuels and renewables in Energy generations. These factors are inportant because Enengy generation is the one of the largest contributor to pollution globally.\n",
    "    ** More use of fossil fuels in Energy Generation shall lead to more pollution.\n",
    "    ** More use of Renewables in Energy Generation shall lead to less pollution.\n",
    "    ** Fossil Fuel, Renewables and Traffic all together contribute to pollution, which in turn contributes to Global Warming.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Depiction of the Bayesian Network and associated probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://www.dropbox.com/s/s1tupltugjdulwe/BayesNet.PNG?raw=1\" alt=\"data1\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the node objects for every random variable as in above diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_node        =   BayesNode('AI', '', 0.2)\n",
    "employed_node =    BayesNode('Employed', '', 0.70)\n",
    "fossil_fuel_node = BayesNode('FossilFuel', '', 0.70)\n",
    "renewables_node =  BayesNode('Renewables', '', 0.40)\n",
    "\n",
    "\n",
    "traffic_node = BayesNode('Traffic', ['AI', 'Employed'], \n",
    "                       {(True, True): 0.65,(True, False): 0.30, (False, True): 0.95, (False, False): 0.35})\n",
    "\n",
    "global_warming_node = BayesNode('Global Warming', ['FossilFuel', 'Traffic', 'Renewables'], \n",
    "                       {(True,True, True): 0.80,(True,True, False): 0.98, (True,False, True): 0.04, (True,False, False): 0.70,\n",
    "                        (False,True, True): 0.30,(False,True, False): 0.45, (False,False, True): 0.20, (False,False, False): 0.10\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020000000000000018"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_warming_node.p(False, {'FossilFuel':True, 'Traffic':True, 'Renewables':False, 'AI':True,'Employed':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_node.p(False,{})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Bayesian Network based on the probabilities from the parents nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T, F = True, False\n",
    "\n",
    "world = BayesNet([\n",
    "    ('AI', '', 0.2),\n",
    "    ('Employed', '', 0.7),\n",
    "    ('FossilFuel', '', 0.7),\n",
    "    ('Renewables', '', 0.7),\n",
    "    ('Traffic', ['AI', 'Employed'], {(True, True): 0.65,(True, False): 0.30, (False, True): 0.95, (False, False): 0.35}),\n",
    "                       \n",
    "    \n",
    "    ('Global Warming', ['FossilFuel', 'Traffic', 'Renewables'], {(True,True, True): 0.80,(True,True, False): 0.98, (True,False, True): 0.04, (True,False, False): 0.70,\n",
    "                                                                (False,True, True): 0.30,(False,True, False): 0.45, (False,False, True): 0.20, (False,False, False): 0.10\n",
    "                                                               })\n",
    "                                                               \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is how the world view bayesian network look like, similar to our diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesNet([('AI', ''), ('Employed', ''), ('FossilFuel', ''), ('Renewables', ''), ('Traffic', 'AI Employed'), ('Global Warming', 'FossilFuel Traffic Renewables')])\n"
     ]
    }
   ],
   "source": [
    "print(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Network\n",
    "* Here we are querying the network to provide the probability of `Global Warming` in an ideal world where \n",
    "    there will be no use of fossil fuels for energy generation, more renewable energy and less traffic.\n",
    "* The network says it shall reduce Global warming drastically. :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08800000000000002"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_dist = enumeration_ask('Global Warming', {'FossilFuels': False, 'Traffic': False, 'Renewables':True}, world)\n",
    "ans_dist[True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.NAIVE BAYES\n",
    "## 2.1 DATA\n",
    "* Below data is from the UCI Repo. This data is about the applications for Nursery Schools in United States. The attributes deal with the various factors such as Family Social and Financial background, health etc.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Below is the description of the attributes as stated in the UCI repository.\n",
    "* The hierarchical model ranks nursery-school applications according\n",
    "   to the following concept structure:\n",
    "\n",
    "   ** NURSERY            Evaluation of applications for nursery schools\n",
    "   ** EMPLOY           Employment of parents and child's nursery\n",
    "   *** parents        Parents' occupation\n",
    "   *** has_nurs       Child's nursery\n",
    "   ** STRUCT_FINAN     Family structure and financial standings\n",
    "   ** STRUCTURE      Family structure\n",
    "   *** form         Form of the family\n",
    "   *** children     Number of children\n",
    "   *** housing        Housing conditions\n",
    "   *** finance        Financial standing of the family\n",
    "   ** SOC_HEALTH       Social and health picture of the family\n",
    "   *** social         Social conditions\n",
    "   *** health         Health conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names=['parents','has_nurs','form','children','housing' ,'finance' ,'social','health','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DataSet(nursery): 1000 examples, 9 attributes>\n"
     ]
    }
   ],
   "source": [
    "nursery_dataset= DataSet(examples=None, attrs=range(9), attrnames=attribute_names, target=-1,\n",
    "                 inputs=None, values=None, distance=mean_boolean_error,\n",
    "                 name='nursery', source='', exclude=())\n",
    "\n",
    "\n",
    "print(nursery_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Prior  Probability of each Class:\n",
    "\n",
    "Turning priority ==> 1 and Non Recommended ==> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing two minority classes from the target collumn and turning the target into binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Classes ['priority', 'not_recom']\n",
      "Class of first example: priority\n",
      "Class of first example: 1\n"
     ]
    }
   ],
   "source": [
    "nursery_dataset.remove_examples(\"recommend\")\n",
    "nursery_dataset.remove_examples(\"very_recom\")\n",
    "\n",
    "print('Target Classes',nursery_dataset.values[nursery_dataset.target])\n",
    "\n",
    "print(\"Class of first example:\",nursery_dataset.examples[14][nursery_dataset.target])\n",
    "nursery_dataset.classes_to_numbers()\n",
    "print(\"Class of first example:\",nursery_dataset.examples[14][nursery_dataset.target])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 579, 0: 333})\n",
      "number_of_observations 912\n"
     ]
    }
   ],
   "source": [
    "target_dictionary=defaultdict(int)\n",
    "def add(o):   \n",
    "    target_dictionary[o] += 1\n",
    "\n",
    "\n",
    "# Finding the target Distribution\n",
    "for example in nursery_dataset.examples:\n",
    "    targetval = example[nursery_dataset.target]\n",
    "    target_dictionary[targetval] += 1\n",
    "print(target_dictionary)\n",
    "\n",
    "# calculating the total number of observations\n",
    "number_of_observations=0\n",
    "for count in target_dictionary.values():\n",
    "    number_of_observations+=count \n",
    "    \n",
    "print('number_of_observations',number_of_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Probability of Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.48684210526315 Percent\n"
     ]
    }
   ],
   "source": [
    "priority=1\n",
    "non_recommended=0\n",
    "print(target_dictionary[priority]/number_of_observations*100,'Percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Probability of Not Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.51315789473684 Percent\n"
     ]
    }
   ],
   "source": [
    "print(target_dictionary[non_recommended]/number_of_observations*100,'Percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Probability of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method put the values given into different buckets and returns a dictionary\n",
    "def create_bucket(values):\n",
    "    bucket=defaultdict(int)\n",
    "    for val in values:\n",
    "        bucket[val]+=1\n",
    "    return bucket\n",
    "\n",
    "# A helper function to print probabilities\n",
    "def print_probabilites(target_dictionary):\n",
    "    number_of_observations=0\n",
    "    for count in target_dictionary.values():\n",
    "        number_of_observations+=count \n",
    "    for key in target_dictionary.keys():\n",
    "        print('Probability of ',key,' is ',target_dictionary[key]/number_of_observations*100,'Percent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Name:  parents\n",
      "defaultdict(<class 'int'>, {'usual': 912})\n",
      "Probability of  usual  is  100.0 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  has_nurs\n",
      "defaultdict(<class 'int'>, {'proper': 798, 'less_proper': 114})\n",
      "Probability of  proper  is  87.5 Percent\n",
      "Probability of  less_proper  is  12.5 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  form\n",
      "defaultdict(<class 'int'>, {'complete': 306, 'completed': 196, 'incomplete': 202, 'foster': 208})\n",
      "Probability of  complete  is  33.55263157894737 Percent\n",
      "Probability of  completed  is  21.49122807017544 Percent\n",
      "Probability of  incomplete  is  22.149122807017545 Percent\n",
      "Probability of  foster  is  22.807017543859647 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  children\n",
      "defaultdict(<class 'int'>, {1: 228, 2: 242, 3: 234, 'more': 208})\n",
      "Probability of  1  is  25.0 Percent\n",
      "Probability of  2  is  26.535087719298247 Percent\n",
      "Probability of  3  is  25.657894736842106 Percent\n",
      "Probability of  more  is  22.807017543859647 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  housing\n",
      "defaultdict(<class 'int'>, {'convenient': 290, 'less_conv': 306, 'critical': 316})\n",
      "Probability of  convenient  is  31.798245614035086 Percent\n",
      "Probability of  less_conv  is  33.55263157894737 Percent\n",
      "Probability of  critical  is  34.64912280701755 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  finance\n",
      "defaultdict(<class 'int'>, {'convenient': 448, 'inconv': 464})\n",
      "Probability of  convenient  is  49.122807017543856 Percent\n",
      "Probability of  inconv  is  50.877192982456144 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  social\n",
      "defaultdict(<class 'int'>, {'nonprob': 290, 'slightly_prob': 289, 'problematic': 333})\n",
      "Probability of  nonprob  is  31.798245614035086 Percent\n",
      "Probability of  slightly_prob  is  31.688596491228072 Percent\n",
      "Probability of  problematic  is  36.51315789473684 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  health\n",
      "defaultdict(<class 'int'>, {'priority': 333, 'not_recom': 333, 'recommended': 246})\n",
      "Probability of  priority  is  36.51315789473684 Percent\n",
      "Probability of  not_recom  is  36.51315789473684 Percent\n",
      "Probability of  recommended  is  26.973684210526315 Percent\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_vals = nursery_dataset.values[0]\n",
    "for attribute_name, attribute_index in zip(nursery_dataset.attrnames,nursery_dataset.inputs):\n",
    "    print('Attribute Name: ',attribute_name)\n",
    "    target_v =[example[attribute_index] for example in nursery_dataset.examples]\n",
    "    attribute_dict=create_bucket(target_v)\n",
    "    print(attribute_dict)\n",
    "    print_probabilites(attribute_dict)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Likelihood of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 579, 0: 333})\n",
      "number_of_observations 912\n"
     ]
    }
   ],
   "source": [
    "target_dictionary=defaultdict(int)\n",
    "def add(o):   \n",
    "    target_dictionary[o] += 1\n",
    "#     number_of_observations += 1\n",
    "\n",
    "# target_dictionary=defaultdict(int)\n",
    "number_of_observations=0\n",
    "target_vals = nursery_dataset.values[nursery_dataset.target]\n",
    "\n",
    "for example in nursery_dataset.examples:\n",
    "    targetval = example[nursery_dataset.target]\n",
    "    add(targetval)\n",
    "    \n",
    "print(target_dictionary)\n",
    "number_of_observations=0\n",
    "for count in target_dictionary.values():\n",
    "    number_of_observations+=count \n",
    "    \n",
    "print('number_of_observations',number_of_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['priority', 'not_recom']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "579\n"
     ]
    }
   ],
   "source": [
    "print(nursery_dataset.values[nursery_dataset.target])\n",
    "print(nursery_dataset.inputs)\n",
    "a=[example[1] for example in nursery_dataset.examples if example[nursery_dataset.target]==1]\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method calculates the likelihood of evidence given the dataset object\n",
    "def calculate_likekihood_ofEvidence(nursery_dataset):\n",
    "\n",
    "    likelihood_evidence_dict=defaultdict(int)\n",
    "    possible_target_class=[0, 1]\n",
    "\n",
    "    # indexes of input features\n",
    "    input_feature_indexes=nursery_dataset.inputs\n",
    "\n",
    "    \n",
    "\n",
    "    # For every feature index, filter the collumn based on the corresponding target class \n",
    "    # Put every filtered data point into respective bucket and return the dictionary of counts for each\n",
    "    # feature class given the target class\n",
    "    for i in input_feature_indexes:\n",
    "        for c in possible_target_class:\n",
    "            relevent_examples=[example[i] for example in nursery_dataset.examples if example[nursery_dataset.target]==c]\n",
    "\n",
    "            for r_example in relevent_examples:\n",
    "                \n",
    "                likelihood_evidence_dict[(i,r_example,c)]+=1\n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "    return likelihood_evidence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(0, 'usual', 0): 333,\n",
       "             (0, 'usual', 1): 579,\n",
       "             (1, 'proper', 0): 288,\n",
       "             (1, 'less_proper', 0): 45,\n",
       "             (1, 'proper', 1): 510,\n",
       "             (1, 'less_proper', 1): 69,\n",
       "             (2, 'complete', 0): 117,\n",
       "             (2, 'completed', 0): 72,\n",
       "             (2, 'incomplete', 0): 72,\n",
       "             (2, 'foster', 0): 72,\n",
       "             (2, 'complete', 1): 189,\n",
       "             (2, 'completed', 1): 124,\n",
       "             (2, 'incomplete', 1): 130,\n",
       "             (2, 'foster', 1): 136,\n",
       "             (3, 1, 0): 90,\n",
       "             (3, 2, 0): 90,\n",
       "             (3, 3, 0): 81,\n",
       "             (3, 'more', 0): 72,\n",
       "             (3, 1, 1): 138,\n",
       "             (3, 2, 1): 152,\n",
       "             (3, 3, 1): 153,\n",
       "             (3, 'more', 1): 136,\n",
       "             (4, 'convenient', 0): 114,\n",
       "             (4, 'less_conv', 0): 111,\n",
       "             (4, 'critical', 0): 108,\n",
       "             (4, 'convenient', 1): 176,\n",
       "             (4, 'less_conv', 1): 195,\n",
       "             (4, 'critical', 1): 208,\n",
       "             (5, 'convenient', 0): 168,\n",
       "             (5, 'inconv', 0): 165,\n",
       "             (5, 'convenient', 1): 280,\n",
       "             (5, 'inconv', 1): 299,\n",
       "             (6, 'nonprob', 0): 111,\n",
       "             (6, 'slightly_prob', 0): 111,\n",
       "             (6, 'problematic', 0): 111,\n",
       "             (6, 'nonprob', 1): 179,\n",
       "             (6, 'slightly_prob', 1): 178,\n",
       "             (6, 'problematic', 1): 222,\n",
       "             (7, 'not_recom', 0): 333,\n",
       "             (7, 'priority', 1): 333,\n",
       "             (7, 'recommended', 1): 246})"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likekihood_ofEvidence(nursery_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to calculate the Likelihood of evidence\n",
    "### Configure the variables `input_index`  `input_class` `target_class` to find the respective likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 579, 0: 333})\n",
      "Numerator: 45\n",
      "Denominator:  333\n",
      "The Likelihood for Input Index:1 Input Class:less_proper Target Class:0 is 0.13513513513513514\n"
     ]
    }
   ],
   "source": [
    "likelihood_dict=calculate_likekihood_ofEvidence(nursery_dataset)\n",
    "target_bucket=create_bucket( [example[nursery_dataset.target] for example in nursery_dataset.examples])\n",
    "print(target_bucket)\n",
    "\n",
    "# Index number of the input feature\n",
    "input_index=1\n",
    "\n",
    "# Input feature class for the above input feature\n",
    "input_class='less_proper'\n",
    "\n",
    "# Target class [0,1]\n",
    "target_class=0\n",
    "\n",
    "print('Numerator:',likelihood_dict[(input_index,input_class,target_class)])\n",
    "print('Denominator: ',target_bucket[target_class])\n",
    "likelihood=likelihood_dict[(input_index,input_class,target_class)] /target_bucket[target_class]\n",
    "print('The Likelihood for Input Index:{0} Input Class:{1} Target Class:{2} is {3}'.format(input_index,input_class,target_class,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "1. Age of patient at time of operation (numerical)\n",
    "2. Patient's year of operation (year - 1900, numerical)\n",
    "3. Number of positive axillary nodes detected (numerical)\n",
    "4. Survival status (class attribute)\n",
    "-- 1 = the patient survived 5 years or longer\n",
    "-- 2 = the patient died within 5 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names=['Age','Year','Nodes_Detected','status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DataSet(haberman): 306 examples, 4 attributes>\n"
     ]
    }
   ],
   "source": [
    "haberman_dataset= DataSet(examples=None, attrs=range(4), attrnames=attribute_names, target=-1,\n",
    "                 inputs=None, values=None, distance=mean_boolean_error,\n",
    "                 name='haberman', source='', exclude=())\n",
    "\n",
    "\n",
    "print(haberman_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Prior  Probability of each Class:\n",
    "\n",
    "Survived More than 5 years post surgery ==> 1 \n",
    "Survived less than 5 years post surgery  ==> 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 225, 2: 81})\n",
      "number_of_observations 306\n"
     ]
    }
   ],
   "source": [
    "target_dictionary=defaultdict(int)\n",
    "def add(o):   \n",
    "    target_dictionary[o] += 1\n",
    "\n",
    "\n",
    "# Finding the target Distribution\n",
    "for example in haberman_dataset.examples:\n",
    "    targetval = example[haberman_dataset.target]\n",
    "    target_dictionary[targetval] += 1\n",
    "print(target_dictionary)\n",
    "\n",
    "# calculating the total number of observations\n",
    "number_of_observations=0\n",
    "for count in target_dictionary.values():\n",
    "    number_of_observations+=count \n",
    "    \n",
    "print('number_of_observations',number_of_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prior Probability of Survived More than 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.52941176470588 Percent\n"
     ]
    }
   ],
   "source": [
    "Survived=1\n",
    "Not_survived=2\n",
    "print(target_dictionary[Survived]/number_of_observations*100,'Percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Probability of Not Recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.47058823529412 Percent\n"
     ]
    }
   ],
   "source": [
    "print(target_dictionary[Not_survived]/number_of_observations*100,'Percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Probability of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method put the values given into different buckets and returns a dictionary\n",
    "def create_bucket(values):\n",
    "    bucket=defaultdict(int)\n",
    "    for val in values:\n",
    "        bucket[val]+=1\n",
    "    return bucket\n",
    "\n",
    "# A helper function to print probabilities\n",
    "def print_probabilites(target_dictionary):\n",
    "    number_of_observations=0\n",
    "    for count in target_dictionary.values():\n",
    "        number_of_observations+=count \n",
    "    for key in target_dictionary.keys():\n",
    "        print('Probability of ',key,' is ',target_dictionary[key]/number_of_observations*100,'Percent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Name:  Age\n",
      "defaultdict(<class 'int'>, {30: 3, 31: 2, 33: 2, 34: 7, 35: 2, 36: 2, 37: 6, 38: 10, 39: 6, 40: 3, 41: 10, 42: 9, 43: 11, 44: 7, 45: 9, 46: 7, 47: 11, 48: 7, 49: 10, 50: 12, 51: 6, 52: 14, 53: 11, 54: 13, 55: 10, 56: 7, 57: 11, 58: 7, 59: 8, 60: 6, 61: 9, 62: 7, 63: 8, 64: 5, 65: 10, 66: 5, 67: 6, 68: 2, 69: 4, 70: 7, 71: 1, 72: 4, 73: 2, 74: 2, 75: 1, 76: 1, 77: 1, 78: 1, 83: 1})\n",
      "Probability of  30  is  0.9803921568627451 Percent\n",
      "Probability of  31  is  0.6535947712418301 Percent\n",
      "Probability of  33  is  0.6535947712418301 Percent\n",
      "Probability of  34  is  2.287581699346405 Percent\n",
      "Probability of  35  is  0.6535947712418301 Percent\n",
      "Probability of  36  is  0.6535947712418301 Percent\n",
      "Probability of  37  is  1.9607843137254901 Percent\n",
      "Probability of  38  is  3.2679738562091507 Percent\n",
      "Probability of  39  is  1.9607843137254901 Percent\n",
      "Probability of  40  is  0.9803921568627451 Percent\n",
      "Probability of  41  is  3.2679738562091507 Percent\n",
      "Probability of  42  is  2.941176470588235 Percent\n",
      "Probability of  43  is  3.594771241830065 Percent\n",
      "Probability of  44  is  2.287581699346405 Percent\n",
      "Probability of  45  is  2.941176470588235 Percent\n",
      "Probability of  46  is  2.287581699346405 Percent\n",
      "Probability of  47  is  3.594771241830065 Percent\n",
      "Probability of  48  is  2.287581699346405 Percent\n",
      "Probability of  49  is  3.2679738562091507 Percent\n",
      "Probability of  50  is  3.9215686274509802 Percent\n",
      "Probability of  51  is  1.9607843137254901 Percent\n",
      "Probability of  52  is  4.57516339869281 Percent\n",
      "Probability of  53  is  3.594771241830065 Percent\n",
      "Probability of  54  is  4.248366013071895 Percent\n",
      "Probability of  55  is  3.2679738562091507 Percent\n",
      "Probability of  56  is  2.287581699346405 Percent\n",
      "Probability of  57  is  3.594771241830065 Percent\n",
      "Probability of  58  is  2.287581699346405 Percent\n",
      "Probability of  59  is  2.6143790849673203 Percent\n",
      "Probability of  60  is  1.9607843137254901 Percent\n",
      "Probability of  61  is  2.941176470588235 Percent\n",
      "Probability of  62  is  2.287581699346405 Percent\n",
      "Probability of  63  is  2.6143790849673203 Percent\n",
      "Probability of  64  is  1.6339869281045754 Percent\n",
      "Probability of  65  is  3.2679738562091507 Percent\n",
      "Probability of  66  is  1.6339869281045754 Percent\n",
      "Probability of  67  is  1.9607843137254901 Percent\n",
      "Probability of  68  is  0.6535947712418301 Percent\n",
      "Probability of  69  is  1.3071895424836601 Percent\n",
      "Probability of  70  is  2.287581699346405 Percent\n",
      "Probability of  71  is  0.32679738562091504 Percent\n",
      "Probability of  72  is  1.3071895424836601 Percent\n",
      "Probability of  73  is  0.6535947712418301 Percent\n",
      "Probability of  74  is  0.6535947712418301 Percent\n",
      "Probability of  75  is  0.32679738562091504 Percent\n",
      "Probability of  76  is  0.32679738562091504 Percent\n",
      "Probability of  77  is  0.32679738562091504 Percent\n",
      "Probability of  78  is  0.32679738562091504 Percent\n",
      "Probability of  83  is  0.32679738562091504 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  Year\n",
      "defaultdict(<class 'int'>, {64: 31, 62: 23, 65: 28, 59: 27, 58: 36, 60: 28, 66: 28, 61: 26, 67: 25, 63: 30, 69: 11, 68: 13})\n",
      "Probability of  64  is  10.130718954248366 Percent\n",
      "Probability of  62  is  7.516339869281046 Percent\n",
      "Probability of  65  is  9.15032679738562 Percent\n",
      "Probability of  59  is  8.823529411764707 Percent\n",
      "Probability of  58  is  11.76470588235294 Percent\n",
      "Probability of  60  is  9.15032679738562 Percent\n",
      "Probability of  66  is  9.15032679738562 Percent\n",
      "Probability of  61  is  8.49673202614379 Percent\n",
      "Probability of  67  is  8.169934640522875 Percent\n",
      "Probability of  63  is  9.803921568627452 Percent\n",
      "Probability of  69  is  3.594771241830065 Percent\n",
      "Probability of  68  is  4.248366013071895 Percent\n",
      "\n",
      "\n",
      "\n",
      "Attribute Name:  Nodes_Detected\n",
      "defaultdict(<class 'int'>, {1: 41, 3: 20, 0: 136, 2: 20, 4: 13, 10: 3, 9: 6, 30: 1, 7: 7, 13: 5, 6: 7, 15: 3, 21: 1, 11: 4, 5: 6, 23: 3, 8: 7, 20: 2, 52: 1, 14: 4, 19: 3, 16: 1, 12: 2, 24: 1, 46: 1, 18: 1, 22: 3, 35: 1, 17: 1, 25: 1, 28: 1})\n",
      "Probability of  1  is  13.398692810457517 Percent\n",
      "Probability of  3  is  6.535947712418301 Percent\n",
      "Probability of  0  is  44.44444444444444 Percent\n",
      "Probability of  2  is  6.535947712418301 Percent\n",
      "Probability of  4  is  4.248366013071895 Percent\n",
      "Probability of  10  is  0.9803921568627451 Percent\n",
      "Probability of  9  is  1.9607843137254901 Percent\n",
      "Probability of  30  is  0.32679738562091504 Percent\n",
      "Probability of  7  is  2.287581699346405 Percent\n",
      "Probability of  13  is  1.6339869281045754 Percent\n",
      "Probability of  6  is  2.287581699346405 Percent\n",
      "Probability of  15  is  0.9803921568627451 Percent\n",
      "Probability of  21  is  0.32679738562091504 Percent\n",
      "Probability of  11  is  1.3071895424836601 Percent\n",
      "Probability of  5  is  1.9607843137254901 Percent\n",
      "Probability of  23  is  0.9803921568627451 Percent\n",
      "Probability of  8  is  2.287581699346405 Percent\n",
      "Probability of  20  is  0.6535947712418301 Percent\n",
      "Probability of  52  is  0.32679738562091504 Percent\n",
      "Probability of  14  is  1.3071895424836601 Percent\n",
      "Probability of  19  is  0.9803921568627451 Percent\n",
      "Probability of  16  is  0.32679738562091504 Percent\n",
      "Probability of  12  is  0.6535947712418301 Percent\n",
      "Probability of  24  is  0.32679738562091504 Percent\n",
      "Probability of  46  is  0.32679738562091504 Percent\n",
      "Probability of  18  is  0.32679738562091504 Percent\n",
      "Probability of  22  is  0.9803921568627451 Percent\n",
      "Probability of  35  is  0.32679738562091504 Percent\n",
      "Probability of  17  is  0.32679738562091504 Percent\n",
      "Probability of  25  is  0.32679738562091504 Percent\n",
      "Probability of  28  is  0.32679738562091504 Percent\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_vals = [1,2]\n",
    "for attribute_name, attribute_index in zip(haberman_dataset.attrnames,haberman_dataset.inputs):\n",
    "    print('Attribute Name: ',attribute_name)\n",
    "    target_v =[example[attribute_index] for example in haberman_dataset.examples]\n",
    "    attribute_dict=create_bucket(target_v)\n",
    "    print(attribute_dict)\n",
    "    print_probabilites(attribute_dict)\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Likelihood of Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 225, 2: 81})\n",
      "number_of_observations 306\n"
     ]
    }
   ],
   "source": [
    "target_dictionary=defaultdict(int)\n",
    "def add(o):   \n",
    "    target_dictionary[o] += 1\n",
    "\n",
    "number_of_observations=0\n",
    "target_vals = haberman_dataset.values[haberman_dataset.target]\n",
    "\n",
    "for example in haberman_dataset.examples:\n",
    "    targetval = example[haberman_dataset.target]\n",
    "    add(targetval)\n",
    "    \n",
    "print(target_dictionary)\n",
    "number_of_observations=0\n",
    "for count in target_dictionary.values():\n",
    "    number_of_observations+=count \n",
    "    \n",
    "print('number_of_observations',number_of_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method calculates the likelihood of evidence given the dataset object\n",
    "def calculate_likekihood_ofEvidence(nursery_dataset):\n",
    "\n",
    "    likelihood_evidence_dict=defaultdict(int)\n",
    "    \n",
    "    possible_target_class = nursery_dataset.values[nursery_dataset.target]\n",
    "\n",
    "    # indexes of input features\n",
    "    input_feature_indexes=nursery_dataset.inputs\n",
    "\n",
    "    \n",
    "\n",
    "    # For every feature index, filter the collumn based on the corresponding target class \n",
    "    # Put every filtered data point into respective bucket and return the dictionary of counts for each\n",
    "    # feature class given the target class\n",
    "    for i in input_feature_indexes:\n",
    "        for c in possible_target_class:\n",
    "            relevent_examples=[example[i] for example in nursery_dataset.examples if example[nursery_dataset.target]==c]\n",
    "\n",
    "            for r_example in relevent_examples:\n",
    "                \n",
    "                likelihood_evidence_dict[(i,r_example,c)]+=1\n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "    return likelihood_evidence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(0, 30, 1): 3,\n",
       "             (0, 31, 1): 2,\n",
       "             (0, 33, 1): 2,\n",
       "             (0, 34, 1): 5,\n",
       "             (0, 35, 1): 2,\n",
       "             (0, 36, 1): 2,\n",
       "             (0, 37, 1): 6,\n",
       "             (0, 38, 1): 9,\n",
       "             (0, 39, 1): 5,\n",
       "             (0, 40, 1): 3,\n",
       "             (0, 41, 1): 7,\n",
       "             (0, 42, 1): 7,\n",
       "             (0, 43, 1): 7,\n",
       "             (0, 44, 1): 4,\n",
       "             (0, 45, 1): 6,\n",
       "             (0, 46, 1): 3,\n",
       "             (0, 47, 1): 8,\n",
       "             (0, 48, 1): 4,\n",
       "             (0, 49, 1): 8,\n",
       "             (0, 50, 1): 10,\n",
       "             (0, 51, 1): 4,\n",
       "             (0, 52, 1): 10,\n",
       "             (0, 53, 1): 5,\n",
       "             (0, 54, 1): 9,\n",
       "             (0, 55, 1): 8,\n",
       "             (0, 56, 1): 5,\n",
       "             (0, 57, 1): 8,\n",
       "             (0, 58, 1): 7,\n",
       "             (0, 59, 1): 7,\n",
       "             (0, 60, 1): 4,\n",
       "             (0, 61, 1): 6,\n",
       "             (0, 62, 1): 4,\n",
       "             (0, 63, 1): 7,\n",
       "             (0, 64, 1): 5,\n",
       "             (0, 65, 1): 6,\n",
       "             (0, 66, 1): 3,\n",
       "             (0, 67, 1): 4,\n",
       "             (0, 68, 1): 2,\n",
       "             (0, 69, 1): 3,\n",
       "             (0, 70, 1): 5,\n",
       "             (0, 71, 1): 1,\n",
       "             (0, 72, 1): 3,\n",
       "             (0, 73, 1): 2,\n",
       "             (0, 74, 1): 1,\n",
       "             (0, 75, 1): 1,\n",
       "             (0, 76, 1): 1,\n",
       "             (0, 77, 1): 1,\n",
       "             (0, 34, 2): 2,\n",
       "             (0, 38, 2): 1,\n",
       "             (0, 39, 2): 1,\n",
       "             (0, 41, 2): 3,\n",
       "             (0, 42, 2): 2,\n",
       "             (0, 43, 2): 4,\n",
       "             (0, 44, 2): 3,\n",
       "             (0, 45, 2): 3,\n",
       "             (0, 46, 2): 4,\n",
       "             (0, 47, 2): 3,\n",
       "             (0, 48, 2): 3,\n",
       "             (0, 49, 2): 2,\n",
       "             (0, 50, 2): 2,\n",
       "             (0, 51, 2): 2,\n",
       "             (0, 52, 2): 4,\n",
       "             (0, 53, 2): 6,\n",
       "             (0, 54, 2): 4,\n",
       "             (0, 55, 2): 2,\n",
       "             (0, 56, 2): 2,\n",
       "             (0, 57, 2): 3,\n",
       "             (0, 59, 2): 1,\n",
       "             (0, 60, 2): 2,\n",
       "             (0, 61, 2): 3,\n",
       "             (0, 62, 2): 3,\n",
       "             (0, 63, 2): 1,\n",
       "             (0, 65, 2): 4,\n",
       "             (0, 66, 2): 2,\n",
       "             (0, 67, 2): 2,\n",
       "             (0, 69, 2): 1,\n",
       "             (0, 70, 2): 2,\n",
       "             (0, 72, 2): 1,\n",
       "             (0, 74, 2): 1,\n",
       "             (0, 78, 2): 1,\n",
       "             (0, 83, 2): 1,\n",
       "             (1, 64, 1): 23,\n",
       "             (1, 62, 1): 16,\n",
       "             (1, 65, 1): 15,\n",
       "             (1, 59, 1): 18,\n",
       "             (1, 58, 1): 24,\n",
       "             (1, 60, 1): 24,\n",
       "             (1, 61, 1): 23,\n",
       "             (1, 67, 1): 21,\n",
       "             (1, 63, 1): 22,\n",
       "             (1, 69, 1): 7,\n",
       "             (1, 66, 1): 22,\n",
       "             (1, 68, 1): 10,\n",
       "             (1, 59, 2): 9,\n",
       "             (1, 66, 2): 6,\n",
       "             (1, 69, 2): 4,\n",
       "             (1, 60, 2): 4,\n",
       "             (1, 64, 2): 8,\n",
       "             (1, 67, 2): 4,\n",
       "             (1, 58, 2): 12,\n",
       "             (1, 63, 2): 8,\n",
       "             (1, 65, 2): 13,\n",
       "             (1, 62, 2): 7,\n",
       "             (1, 68, 2): 3,\n",
       "             (1, 61, 2): 3,\n",
       "             (2, 1, 1): 33,\n",
       "             (2, 3, 1): 13,\n",
       "             (2, 0, 1): 117,\n",
       "             (2, 2, 1): 15,\n",
       "             (2, 4, 1): 10,\n",
       "             (2, 10, 1): 2,\n",
       "             (2, 30, 1): 1,\n",
       "             (2, 7, 1): 5,\n",
       "             (2, 13, 1): 1,\n",
       "             (2, 6, 1): 4,\n",
       "             (2, 15, 1): 1,\n",
       "             (2, 11, 1): 1,\n",
       "             (2, 5, 1): 2,\n",
       "             (2, 8, 1): 5,\n",
       "             (2, 20, 1): 1,\n",
       "             (2, 14, 1): 3,\n",
       "             (2, 16, 1): 1,\n",
       "             (2, 12, 1): 1,\n",
       "             (2, 46, 1): 1,\n",
       "             (2, 19, 1): 1,\n",
       "             (2, 18, 1): 1,\n",
       "             (2, 22, 1): 2,\n",
       "             (2, 9, 1): 2,\n",
       "             (2, 25, 1): 1,\n",
       "             (2, 28, 1): 1,\n",
       "             (2, 0, 2): 19,\n",
       "             (2, 9, 2): 4,\n",
       "             (2, 21, 2): 1,\n",
       "             (2, 23, 2): 3,\n",
       "             (2, 1, 2): 8,\n",
       "             (2, 52, 2): 1,\n",
       "             (2, 2, 2): 5,\n",
       "             (2, 6, 2): 3,\n",
       "             (2, 19, 2): 2,\n",
       "             (2, 3, 2): 7,\n",
       "             (2, 5, 2): 4,\n",
       "             (2, 20, 2): 1,\n",
       "             (2, 11, 2): 3,\n",
       "             (2, 7, 2): 2,\n",
       "             (2, 10, 2): 1,\n",
       "             (2, 13, 2): 4,\n",
       "             (2, 4, 2): 3,\n",
       "             (2, 24, 2): 1,\n",
       "             (2, 12, 2): 1,\n",
       "             (2, 15, 2): 2,\n",
       "             (2, 14, 2): 1,\n",
       "             (2, 35, 2): 1,\n",
       "             (2, 17, 2): 1,\n",
       "             (2, 22, 2): 1,\n",
       "             (2, 8, 2): 2})"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likekihood_ofEvidence(haberman_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to calculate the Likelihood of evidence\n",
    "### Configure the variables `input_index`  `input_class` `target_class` to find the respective likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 225, 2: 81})\n",
      "Numerator: 2\n",
      "Denominator:  81\n",
      "The Likelihood for Input Index:0 Input Class:34 Target Class:2 is 0.024691358024691357\n"
     ]
    }
   ],
   "source": [
    "likelihood_dict=calculate_likekihood_ofEvidence(haberman_dataset)\n",
    "target_bucket=create_bucket( [example[haberman_dataset.target] for example in haberman_dataset.examples])\n",
    "print(target_bucket)\n",
    "\n",
    "# Index number of the input feature\n",
    "input_index=0\n",
    "\n",
    "# Input feature class for the above input feature\n",
    "input_class=34\n",
    "\n",
    "# Target class [1,2]\n",
    "target_class=2\n",
    "\n",
    "print('Numerator:',likelihood_dict[(input_index,input_class,target_class)])\n",
    "print('Denominator: ',target_bucket[target_class])\n",
    "likelihood=likelihood_dict[(input_index,input_class,target_class)] /target_bucket[target_class]\n",
    "print('The Likelihood for Input Index:{0} Input Class:{1} Target Class:{2} is {3}'.format(input_index,input_class,target_class,likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Naive Bayes Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* source learning.ipynb\n",
    "\n",
    "We have a dataset with a set of classes (**C**) and we want to classify an item with a set of features (**F**). Essentially what we want to do is predict the class of an item given the features.\n",
    "\n",
    "For a specific class, **Class**, we will find the conditional probability given the item features:\n",
    "\n",
    "$$P(Class|F) = \\dfrac{P(F|Class)*P(Class)}{P(F)}$$\n",
    "\n",
    "We will do this for every class and we will pick the maximum. This will be the class the item is classified in.\n",
    "\n",
    "The features though are a vector with many elements. We need to break the probabilities up using the multiplication rule. Thus the above equation becomes:\n",
    "\n",
    "$$P(Class|F) = \\dfrac{P(Class)*P(F_{1}|Class)*P(F_{2}|Class)*...*P(F_{n}|Class)}{P(F_{1})*P(F_{2})*...*P(F_{n})}$$\n",
    "\n",
    "The calculation of the conditional probability then depends on the calculation of the following:\n",
    "\n",
    "*a)* The probability of **Class** in the dataset.\n",
    "\n",
    "*b)* The conditional probability of each feature occurring in an item classified in **Class**.\n",
    "\n",
    "*c)* The probabilities of each individual feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Learner\n",
    "\n",
    "## we shall use below functions developed to compute the Likelihood of evidence and Prior Probability to find the probability given the evidence and shall choose the max probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ofEvidence(nursery_dataset,possible_target_class=[0, 1]):\n",
    "\n",
    "    likelihood_evidence_dict=defaultdict(int)\n",
    "    likelihood_evidence_distribution_dict=defaultdict(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    input_feature_indexes=nursery_dataset.inputs\n",
    "\n",
    "    target_bucket=create_bucket( [example[nursery_dataset.target] for example in nursery_dataset.examples])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # counting the feature classes given the evidence from the examples.\n",
    "    for i in input_feature_indexes:\n",
    "        for c in possible_target_class:\n",
    "            relevent_examples=[example[i] for example in nursery_dataset.examples if example[nursery_dataset.target]==c]\n",
    "\n",
    "            for r_example in relevent_examples:\n",
    "                \n",
    "                likelihood_evidence_dict[(i,r_example,c)]+=1\n",
    "    \n",
    "    # For every feature index, filter the collumn based on the corresponding target class \n",
    "    # Put every filtered data point into respective bucket and return the dictionary of counts for each\n",
    "    # feature class given the target class\n",
    "    \n",
    "    for i in input_feature_indexes:\n",
    "        for c in possible_target_class:\n",
    "            relevent_examples=[example[i] for example in nursery_dataset.examples if example[nursery_dataset.target]==c]\n",
    "\n",
    "            for r_example in relevent_examples:\n",
    "                \n",
    "                likelihood_evidence_distribution_dict[(i,r_example,c)]= likelihood_dict[(i,r_example,c)] /target_bucket[c]\n",
    "\n",
    "    \n",
    "#     print(likelihood_evidence_dict)\n",
    "#     print('\\n\\n')\n",
    "#     print(likelihood_evidence_distribution_dict)\n",
    "    return likelihood_evidence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(nursery_dataset,target_class_list=[0,1]):\n",
    "    target_dictionary=defaultdict(int)    \n",
    "\n",
    "    # Finding the target Distribution\n",
    "    for example in nursery_dataset.examples:\n",
    "        targetval = example[nursery_dataset.target]\n",
    "        target_dictionary[targetval] += 1\n",
    "    print(target_dictionary)\n",
    "\n",
    "    # calculating the total number of observations\n",
    "    number_of_observations=0\n",
    "    for count in target_dictionary.values():\n",
    "        number_of_observations+=count \n",
    "\n",
    "    print('number_of_observations',number_of_observations)\n",
    "    \n",
    "    # calculating the probability of each target class\n",
    "    target_distribution_dictionary=defaultdict(int)\n",
    "    \n",
    "    # calculating in percentage terms\n",
    "    for target_class in target_class_list:\n",
    "        target_distribution_dictionary[target_class]=target_dictionary[target_class]/number_of_observations\n",
    "        \n",
    "    print(target_distribution_dictionary)\n",
    "    return target_distribution_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Target and Attribute distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 579, 0: 333})\n",
      "number_of_observations 912\n",
      "defaultdict(<class 'int'>, {0: 0.3651315789473684, 1: 0.6348684210526315})\n"
     ]
    }
   ],
   "source": [
    "attr_dists=likelihood_ofEvidence(nursery_dataset,possible_target_class=[0, 1])\n",
    "\n",
    "target_dist=target_distribution(nursery_dataset,target_class_list=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse_data_point(data):\n",
    "    \n",
    "    test_list=data.split(sep=',')\n",
    "    target=test_list[-1]\n",
    "\n",
    "    print(test_list)\n",
    "    test_list.pop()\n",
    "    print(test_list)\n",
    "    return [test_list,target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Target Class for given example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usual', 'improper', 'complete', '3', 'convenient', 'convenient', 'nonprob', 'not_recom', 'not_recom']\n",
      "['usual', 'improper', 'complete', '3', 'convenient', 'convenient', 'nonprob', 'not_recom']\n",
      "Expected: not_recom\n",
      "Predicted: priority\n"
     ]
    }
   ],
   "source": [
    "def predict(example):\n",
    "    def class_probability(targetval):\n",
    "        return (target_dist[targetval] *\n",
    "                product(attr_dists[(attr,example[attr],targetval )]\n",
    "                        for attr in nursery_dataset.inputs))\n",
    "    return argmax(target_vals, key=class_probability)\n",
    "\n",
    "\n",
    "# example1=['usual','proper','complete',1,'convenient','convenient','nonprob','priority']\n",
    "# example2=['usual','proper','complete','2','critical','convenient','problematic','recommended','priority']\n",
    "# example3=['usual','proper','completed','1','convenient','convenient','nonprob','not_recom']\n",
    "\n",
    "\n",
    "results=[]\n",
    "data_point='usual,improper,complete,3,convenient,convenient,nonprob,not_recom,not_recom'\n",
    "data=parse_data_point(data_point)\n",
    "\n",
    "# a = map(str, [usual,proper,completed,1,convenient,convenient,nonprob,not_recom,not_recom])\n",
    "predicted_target_class=predict(data[0])\n",
    "expected_class=data[1]\n",
    "results.append([expected_class,predicted_target_class])\n",
    "print('Expected:',data[1])\n",
    "\n",
    "if predicted_target_class==1:\n",
    "    print('Predicted: priority')\n",
    "\n",
    "else:\n",
    "    print('Predicted: not_recom')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Learner with a test data set\n",
    "\n",
    "`url` https://archive.ics.uci.edu/ml/machine-learning-databases/nursery/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DataSet(nursery_test): 1000 examples, 9 attributes>\n",
      "['priority', 'not_recom']\n",
      "Class of first example: priority\n",
      "Class of first example: 1\n"
     ]
    }
   ],
   "source": [
    "attribute_names=['parents','has_nurs','form','children','housing' ,'finance' ,'social','health','target']\n",
    "nursery_test_dataset= DataSet(examples=None, attrs=range(9), attrnames=attribute_names, target=-1,\n",
    "                 inputs=None, values=None, distance=mean_boolean_error,\n",
    "                 name='nursery_test', source='', exclude=())\n",
    "\n",
    "\n",
    "print(nursery_test_dataset)\n",
    "nursery_test_dataset.remove_examples(\"recommend\")\n",
    "nursery_test_dataset.remove_examples(\"very_recom\")\n",
    "\n",
    "print(nursery_test_dataset.values[nursery_test_dataset.target])\n",
    "\n",
    "print(\"Class of first example:\",nursery_test_dataset.examples[14][nursery_test_dataset.target])\n",
    "nursery_test_dataset.classes_to_numbers()\n",
    "print(\"Class of first example:\",nursery_test_dataset.examples[14][nursery_test_dataset.target])\n",
    "# print(nursery_test_dataset.values[nursery_test_dataset.target])\n",
    "# print(len(nursery_test_dataset.values[0]))\n",
    "# print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the learner on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Accuracy of the Model: 0.6392199349945829\n"
     ]
    }
   ],
   "source": [
    "# data=parse_data_point(nursery_test_dataset.examples[0])\n",
    "results=[]\n",
    "for example in nursery_test_dataset.examples:\n",
    "\n",
    "    expected_class=example[-1]\n",
    "#     if expected_class==1:\n",
    "#         expected_class='priority'\n",
    "#     else:\n",
    "#         expected_class='not_recom'\n",
    "    \n",
    "    predicted_target_class=predict(example)\n",
    "\n",
    "    results.append([expected_class,predicted_target_class])\n",
    "    \n",
    "\n",
    "right_prediction_list = [result for result in results if result[0]==result[1]]\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "accuracy=len(right_prediction_list)/len(results)\n",
    "\n",
    "print('Accuracy of the Model:',accuracy)\n",
    "\n",
    "# Accuracy of the Model: 0.6392199349945829\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continious Learner\n",
    "* We shall use this data set to train and validate a Naive Bayes Continious Leaner which shall take continous value \n",
    "    and predict the target class.\n",
    "* Continious Learners are different from Discrete as continous data may end up creating a huge set of likelihood  probabilities hence many distribution buckets and hence poor performance.\n",
    "* We shall here assume every data to be Gaussian ( central limit theorem ) and hence use mean and standard deviation to predict the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set is a Audit Data set which contain information about the companies based on industries and their history.\n",
    "* Attributes are used to predict if the company is a RISK or not .\n",
    "* Many risk factors are examined from various areas like past records of audit office, audit-paras, environmental \n",
    "  conditions reports, firm reputation summary, on-going issues report, profit-value records, loss-value records,\n",
    "  follow-up reports etc. After in-depth interview with the auditors, important risk factors are evaluated and\n",
    "  their probability of existence is calculated from the present and past records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "<DataSet(audit_risk_data): 771 examples, 18 attributes>\n",
      "[0, 1]\n",
      "[3.89, 6, 0, 2, 4.83, 2, 4.83, 5, 2, 0.94, 2, 2, 0, 2, 0, 2, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "attribute_names= ['Sector_score','LOCATION_ID','PARA_A','SCORE_A','PARA_B','SCORE_B','TOTAL','numbers','Marks','Money_Value','MONEY_Marks','District','Loss','LOSS_SCORE','History','History_score','Score','Risk']\n",
    "\n",
    "\n",
    "\n",
    "print(len(attribute_names))\n",
    "audit_risk_dataset= DataSet(examples=None, attrs=range(18), attrnames=attribute_names, target=-1,\n",
    "                 inputs=None, values=None, distance=mean_boolean_error,\n",
    "                 name='audit_risk_data', source='', exclude=())\n",
    "\n",
    "\n",
    "print(audit_risk_dataset)\n",
    "\n",
    "\n",
    "print(audit_risk_dataset.values[audit_risk_dataset.target])\n",
    "\n",
    "print(audit_risk_dataset.examples[0])\n",
    "# print(audit_risk_dataset.target)\n",
    "\n",
    "# print(audit_risk_dataset.values[17])\n",
    "# print(len(audit_risk_dataset.values))\n",
    "# print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values_by_classes(iaudit_risk_dataset):\n",
    "        \"\"\"Split values into buckets according to their class.\"\"\"\n",
    "        buckets = defaultdict(list)\n",
    "\n",
    "        for v in iaudit_risk_dataset.examples:\n",
    "            target=v[-1]\n",
    "            buckets[target].append(v[:-1])  # Add item to bucket of its class\n",
    "\n",
    "        return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_means_and_deviations(dataset,item_buckets):\n",
    "    # Find the mean and standard deviation of each input feature class given the target class.\n",
    "    \n",
    "       \n",
    "        target_names = dataset.values[dataset.target]\n",
    "        feature_numbers = len(dataset.inputs)\n",
    "\n",
    "\n",
    "        # initialize the dictionary of mean and deviations\n",
    "        means = defaultdict(lambda: [0] * feature_numbers)\n",
    "        deviations = defaultdict(lambda: [0] * feature_numbers)\n",
    "\n",
    "\n",
    "        for t in target_names:\n",
    "            # Find all the item feature values for item in class t\n",
    "            features = [[] for i in range(feature_numbers)]\n",
    "            for item in item_buckets[t]:\n",
    "                for i in dataset.inputs:\n",
    "                    features[i].append(item[i])\n",
    "\n",
    "            # Calculate means and deviations of  the class\n",
    "            for i in range(17):\n",
    "                means[t][i] = mean(features[i])\n",
    "                deviations[t][i] = stdev(features[i])\n",
    "\n",
    "        return means, deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************means: defaultdict(<function find_means_and_deviations.<locals>.<lambda> at 0x00000252DCF30948>, {0: [32.20174825174825, 14.265734265734265, 0.30055944055944056, 2, 0.25454895104895103, 2, 0.5551083916083916, 5, 2, 0.2635454545454545, 2, 2, 0.001, 2, 0.001, 2, 2], 1: [13.171278350515465, 15.210309278350515, 3.731238350515464, 4.412371134020619, 17.124721649484535, 3.8103092783505152, 20.805238350515463, 5.108247422680412, 2.379381443298969, 22.426329896907216, 3.4556701030927837, 2.808247422680412, 0.04742268041237113, 2.0989690721649485, 0.1670103092783505, 2.268041237113402, 3.123298969072165]})\n",
      "\n",
      "\n",
      "\n",
      "***************devations: defaultdict(<function find_means_and_deviations.<locals>.<lambda> at 0x00000252DD0C93A8>, {0: [25.1447752255497, 9.460871254676208, 0.3088047727650804, 0.001, 0.4944437146924773, 0.001, 0.5845905933421018, 0.001, 0.001, 0.6904314006324352, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001], 1: [20.853889048450608, 10.131016573127088, 6.867803367921, 1.6297377046451043, 62.52561514244347, 1.8399539910751135, 63.73372732945552, 0.32798851745320845, 0.9898178044562412, 83.12843559627674, 1.813799780298059, 1.4737139023325596, 0.23136889773672686, 0.4707333046246197, 0.6641221213318071, 0.8444699046050075, 0.8414208782274878]})\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data set into buckets based on input feature classes\n",
    "a=split_values_by_classes(audit_risk_dataset)\n",
    "\n",
    "# Finding mean and devuation for each input class\n",
    "means,deviations=find_means_and_deviations(audit_risk_dataset,a)\n",
    "\n",
    "\n",
    "# This was major challlenge i faced while predicting the results\n",
    "# It is important that the zero values are replaced with a minute value to allow calculations\n",
    "for key,values  in means.items():\n",
    "    for i,value in zip( range(len(values)) , values ):\n",
    "        if value==0 or value==0.0:\n",
    "\n",
    "            means[key][i]=0.001\n",
    "for key,values  in deviations.items():\n",
    "    for i,value in zip( range(len(values)) , values ):\n",
    "        if value==0 or value==0.0:\n",
    "            deviations[key][i]=0.001\n",
    "            \n",
    "\n",
    "print('***************means:',means)\n",
    "print('\\n\\n')\n",
    "print('***************devations:',deviations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 286, 1: 485})\n",
      "number_of_observations 771\n",
      "defaultdict(<class 'int'>, {0: 0.37094682230869, 1: 0.62905317769131})\n"
     ]
    }
   ],
   "source": [
    "attr_dists_cont=likelihood_ofEvidence(audit_risk_dataset,possible_target_class=[0, 1])\n",
    "\n",
    "target_dist_cont=target_distribution(audit_risk_dataset,target_class_list=[0,1])\n",
    "target_vals=[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def predict_continious(example):\n",
    "    def class_probability(targetval):\n",
    "        prob = target_dist_cont[targetval]\n",
    "        for attr in audit_risk_dataset.inputs:\n",
    "            prob *= gaussian(means[targetval][attr], deviations[targetval][attr], example[attr])\n",
    "        return prob\n",
    "\n",
    "    return argmax(target_vals, key=class_probability)\n",
    "\n",
    "\n",
    "print(predict_continious([3.89,7,1.1,4,7.41,4,8.51,5,2,44.95,6,2,0,2,0,2,3.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "<DataSet(audit_risk_data_test): 75 examples, 18 attributes>\n",
      "[0, 1]\n",
      "[1.85, 6, 3.7, 6, 0, 2, 3.7, 5, 2, 0.12, 2, 2, 0, 2, 0, 2, 2.4, 1]\n"
     ]
    }
   ],
   "source": [
    "attribute_names= ['Sector_score','LOCATION_ID','PARA_A','SCORE_A','PARA_B','SCORE_B','TOTAL','numbers','Marks','Money_Value','MONEY_Marks','District','Loss','LOSS_SCORE','History','History_score','Score','Risk']\n",
    "\n",
    "\n",
    "\n",
    "print(len(attribute_names))\n",
    "audit_risk_test_dataset= DataSet(examples=None, attrs=range(18), attrnames=attribute_names, target=-1,\n",
    "                 inputs=None, values=None, distance=mean_boolean_error,\n",
    "                 name='audit_risk_data_test', source='', exclude=())\n",
    "\n",
    "\n",
    "print(audit_risk_test_dataset)\n",
    "\n",
    "\n",
    "print(audit_risk_test_dataset.values[audit_risk_test_dataset.target])\n",
    "\n",
    "print(audit_risk_test_dataset.examples[0])\n",
    "# print(audit_risk_test_dataset.target)\n",
    "\n",
    "# print(audit_risk_test_dataset.values[17])\n",
    "# print(len(audit_risk_test_dataset.values))\n",
    "# print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "predicted_target_class=predict_continious([3.89,7,1.1,4,7.41,4,8.51,5,2,44.95,6,2,0,2,0,2,3.2])\n",
    "print(predicted_target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], [1, 1], [1, 1], [0, 0], [1, 1], [1, 1], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "\n",
      "\n",
      "\n",
      "Accuracy of the Model: 1.0\n"
     ]
    }
   ],
   "source": [
    "# data=parse_data_point(audit_risk_test_dataset.examples[0])\n",
    "results=[]\n",
    "for example in audit_risk_test_dataset.examples:\n",
    "    expected_class=example[-1]    \n",
    "    predicted_target_class=predict_continious(example[:-1])\n",
    "    results.append([expected_class,predicted_target_class])\n",
    "    # print('Expected:',expected_class)\n",
    "print(results)\n",
    "\n",
    "right_prediction_list = [result for result in results if result[0]==result[1]]\n",
    "print('\\n\\n')\n",
    "# print(right_prediction_list)\n",
    "\n",
    "accuracy=len(right_prediction_list)/len(results)\n",
    "\n",
    "print('Accuracy of the Model:',accuracy)\n",
    "# Accuracy of the Model: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This learner produces a 100% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
